{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38e46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# lib for pre trained model \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "# lib for layers\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "# lib for connecting pretrained and created model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "#lib for compile\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# lib for pre processing\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# lib for spliting i/p and o/p\n",
    "from sklearn.model_selection import train_test_split\n",
    "# lib for metrics\n",
    "from sklearn.metrics import classification_report\n",
    "# other lib\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0659a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "# to assign the dataset path\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, help=\"path to input dataset\")\n",
    "# to assign for output\n",
    "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",help=\"path to output loss/accuracy plot\")\n",
    "# for model\n",
    "ap.add_argument(\"-m\", \"--model\", type=str,default=\"mask_detectorch.model\",help=\"path to output face mask detector model\")\n",
    "args, unknown = ap.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e091cb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get the list of images in the dataset directory\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = list(paths.list_images(r'C:/HOPE/Deep Learning/Face/dataset'))\n",
    "data =[]\n",
    "labels = []\n",
    "\n",
    "# loop over image path\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class name from the file name bocz here we have folder data \n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    \n",
    "    # load the input image(224,224) and preprocess it\n",
    "    image = load_img(imagePath, target_size=(224,224))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    #append it to the list created --> data and label\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "    # print(data)\n",
    "    # print(labels)\n",
    "\n",
    "# convert the data and labels lists \n",
    "\n",
    "data = np.array(data,dtype =\"float32\")\n",
    "labels = np.array(labels)\n",
    "# print(data)\n",
    "# print(labels)\n",
    "\n",
    "# perform one hot encoding on labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f04c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "(trainx, testx, trainy, testy)=train_test_split(data,labels,test_size=0.20,\n",
    "                                                stratify=labels,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73966540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct the trianing image generator for data augementation\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range= 0.2,\n",
    "    shear_range= 0.15,\n",
    "    horizontal_flip= True,\n",
    "    fill_mode= 'nearest'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a8a69c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kathi\\AppData\\Local\\Temp\\ipykernel_10132\\2846588376.py:2: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  basemodel = MobileNetV2(weights ='imagenet',include_top = False,\n"
     ]
    }
   ],
   "source": [
    "# load the pre trained  model\n",
    "basemodel = MobileNetV2(weights ='imagenet',include_top = False,\n",
    "                        input_tensor = Input(shape = (224,224,3)))\n",
    "\n",
    "# develope a head model to place in the top of the model\n",
    "headmodel = basemodel.output\n",
    "headmodel = AveragePooling2D(pool_size =(7,7))(headmodel)\n",
    "headmodel = Flatten(name = 'flatten')(headmodel)\n",
    "headmodel = Dense(128,activation ='relu')(headmodel)\n",
    "headmodel = Dropout(0.5)(headmodel)\n",
    "headmodel = Dense(2,activation ='softmax')(headmodel)\n",
    "\n",
    "# building actual model by combining basemodel and headmodel of FC and that will be the training model\n",
    "model = Model(inputs =basemodel.input,outputs = headmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7efcfbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop all over layers in the basemodel and freeze it to stop the update\n",
    "\n",
    "# mandatory step in all pre trained model program\n",
    "\n",
    "for layer in basemodel.layers:\n",
    "    layer.trainable =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b89eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Compiling model....\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "\n",
    "print(\"[INFO] Compiling model....\")\n",
    "# initialize the initial learning rate, number of epochs to train for,\n",
    "# and batch size\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 32\n",
    "opt = Adam(learning_rate=INIT_LR)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer =opt,\n",
    "              metrics =[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2908140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating steps per epoch and validation steps\n",
    "steps_per_epoch = len(trainx)//BS\n",
    "validation_steps = len(testx)//BS\n",
    "\n",
    "# Use .repeat() to ensure the dataset generates enough batches\n",
    "train_data_gen = aug.flow(trainx, trainy, batch_size=BS)\n",
    "validation_data_gen = ImageDataGenerator().flow(testx, testy, batch_size=BS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f99de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training ahead....\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 731ms/step - accuracy: 0.7544 - loss: 0.5828 - val_accuracy: 0.9844 - val_loss: 0.1516\n",
      "Epoch 2/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9062 - loss: 0.2433 - val_accuracy: 1.0000 - val_loss: 0.0612\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 722ms/step - accuracy: 0.9582 - loss: 0.1669 - val_accuracy: 0.9831 - val_loss: 0.0840\n",
      "Epoch 4/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698us/step - accuracy: 0.9688 - loss: 0.1235 - val_accuracy: 1.0000 - val_loss: 0.0101\n",
      "Epoch 5/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 700ms/step - accuracy: 0.9744 - loss: 0.1012 - val_accuracy: 0.9883 - val_loss: 0.0640\n",
      "Epoch 6/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - accuracy: 0.9688 - loss: 0.1531 - val_accuracy: 1.0000 - val_loss: 0.0490\n",
      "Epoch 7/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 697ms/step - accuracy: 0.9753 - loss: 0.0831 - val_accuracy: 0.9883 - val_loss: 0.0577\n",
      "Epoch 8/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 650us/step - accuracy: 1.0000 - loss: 0.0245 - val_accuracy: 1.0000 - val_loss: 0.0037\n",
      "Epoch 9/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 699ms/step - accuracy: 0.9862 - loss: 0.0551 - val_accuracy: 0.9909 - val_loss: 0.0484\n",
      "Epoch 10/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 622us/step - accuracy: 0.9688 - loss: 0.0551 - val_accuracy: 1.0000 - val_loss: 0.0084\n",
      "Epoch 11/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 696ms/step - accuracy: 0.9817 - loss: 0.0651 - val_accuracy: 0.9896 - val_loss: 0.0477\n",
      "Epoch 12/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 550us/step - accuracy: 1.0000 - loss: 0.0443 - val_accuracy: 1.0000 - val_loss: 0.0166\n",
      "Epoch 13/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 702ms/step - accuracy: 0.9879 - loss: 0.0493 - val_accuracy: 0.9896 - val_loss: 0.0418\n",
      "Epoch 14/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692us/step - accuracy: 1.0000 - loss: 0.0186 - val_accuracy: 1.0000 - val_loss: 0.0179\n",
      "Epoch 15/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 696ms/step - accuracy: 0.9877 - loss: 0.0475 - val_accuracy: 0.9909 - val_loss: 0.0416\n",
      "Epoch 16/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 580us/step - accuracy: 1.0000 - loss: 0.0170 - val_accuracy: 1.0000 - val_loss: 0.0024\n",
      "Epoch 17/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 732ms/step - accuracy: 0.9879 - loss: 0.0403 - val_accuracy: 0.9896 - val_loss: 0.0373\n",
      "Epoch 18/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 493us/step - accuracy: 0.9688 - loss: 0.0902 - val_accuracy: 1.0000 - val_loss: 0.0329\n",
      "Epoch 19/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 733ms/step - accuracy: 0.9887 - loss: 0.0457 - val_accuracy: 0.9922 - val_loss: 0.0374\n",
      "Epoch 20/20\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 619us/step - accuracy: 1.0000 - loss: 0.0221 - val_accuracy: 1.0000 - val_loss: 0.0055\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "print(\"[INFO] training ahead....\")\n",
    "H = model.fit(train_data_gen,\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              validation_data = validation_data_gen,\n",
    "              validation_steps=validation_steps,\n",
    "              epochs =EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87c9adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INFO] evaluating model...\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 506ms/step\n"
     ]
    }
   ],
   "source": [
    "# test prediction\n",
    "print(\"['INFO] evaluating model...\")\n",
    "predIdxs =model.predict(testx,batch_size=BS)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "\n",
    "predIdxs = np.argmax(predIdxs, axis =1)\n",
    "# print(predIdxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e67d20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   with_mask       0.99      0.99      0.99       384\n",
      "without_mask       0.99      0.99      0.99       386\n",
      "\n",
      "    accuracy                           0.99       770\n",
      "   macro avg       0.99      0.99      0.99       770\n",
      "weighted avg       0.99      0.99      0.99       770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print(classification_report(testy.argmax(axis =1), predIdxs, target_names=lb.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c1e2e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving the mask detector model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Save the model\n",
    "print(\"[INFO] saving the mask detector model...\")\n",
    "model.save('mask_detector.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1033f1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
